Multi-UAV Simulation Log - Started at 20250501_063147
================================================================================

Configuration Parameters:
  scenario_width: 10.0
  scenario_height: 10.0
  uav_radius: 0.05
  target_radius: 0.05
  obstacle_radius_min: 0.04
  obstacle_radius_max: 0.1
  capture_distance: 1.2
  sensor_range: 0.4
  max_steps_per_episode: 50
  num_sensors: 24
  trajectory_length: 50
  time_step: 1
  uav_initial_velocity: 0.0
  uav_max_velocity: 0.13
  uav_max_acceleration: 0.05
  uav_mass: 0.5
  target_initial_velocity: 0.0
  target_max_velocity: 0.13
  target_max_acceleration: 0.05
  replay_buffer_size: 100000
  batch_size: 256
  pre_batch_size: 50
  gamma: 0.99
  tau: 0.001
  actor_lr: 0.0005
  critic_lr: 0.001
  num_episodes: 30000
  epsilon: 1.0
  epsilon_min: 0.01
  epsilon_decay: 0.98
  d_limit: 1.5
  curriculum_threshold: 5000
  curriculum_step_size: 1000
  curriculum_learning: {'reward_weights': {'approach': 1.0, 'safety': 0.5, 'track': 0.3, 'encircle': 0.5, 'capture': 1.0, 'finish': 10.0}, 'curriculum_steps': [{'step': 0, 'reward_weights': {'approach': 1.0, 'safety': 0.5, 'track': 0.3, 'encircle': 0.5, 'capture': 1.0, 'finish': 10.0}}, {'step': 1000, 'reward_weights': {'approach': 0.8, 'safety': 0.6, 'track': 0.4, 'encircle': 0.6, 'capture': 1.2, 'finish': 12.0}}, {'step': 2000, 'reward_weights': {'approach': 0.6, 'safety': 0.7, 'track': 0.5, 'encircle': 0.7, 'capture': 1.4, 'finish': 14.0}}, {'step': 3000, 'reward_weights': {'approach': 0.4, 'safety': 0.8, 'track': 0.6, 'encircle': 0.8, 'capture': 1.6, 'finish': 16.0}}, {'step': 4000, 'reward_weights': {'approach': 0.2, 'safety': 0.9, 'track': 0.7, 'encircle': 0.9, 'capture': 1.8, 'finish': 18.0}}]}
  correlation_weights: {'sigma1': 100, 'sigma2': 2, 'sigma3': 5}
  reward_weights: {'approach': 1.0, 'safety': 1.0, 'track': 1.0, 'encircle': 1.0, 'capture': 1.0, 'finish': 1.0}

Episode 0 - Started (Seed: 7315)
  Initial Positions:
    seed: 7315
ERROR: Curriculum learning: Episode 0, placing target closer to UAVs (min_distance=0.25)
Episode 0 - Started (Seed: 7315)
  Initial Positions:
    uavs: [[2.8451949116508244, 4.344829764614092], [9.180592303558381, 2.8998648472812874], [0.6185455567226418, 6.290084411000242]]
    target: [0.1804074949452772, 1.4258838740104947]
    obstacles: {'positions': [[4.897119084549709, 2.7074151153573687], [6.817196954568747, 4.561579339511071], [3.7013928119888773, 8.541530474951395]], 'radii': [0.04855874348517867, 0.06856799217541762, 0.06906927703972196]}
Episode 0 - Started (Seed: 7315)
  Initial Positions:
    seed: 7315
ERROR: Curriculum learning: Episode 0, placing target closer to UAVs (min_distance=0.25)
Episode 0 - Started (Seed: 7315)
  Initial Positions:
    uavs: [[2.8451949116508244, 4.344829764614092], [9.180592303558381, 2.8998648472812874], [0.6185455567226418, 6.290084411000242]]
    target: [0.1804074949452772, 1.4258838740104947]
    obstacles: {'positions': [[4.897119084549709, 2.7074151153573687], [6.817196954568747, 4.561579339511071], [3.7013928119888773, 8.541530474951395]], 'radii': [0.04855874348517867, 0.06856799217541762, 0.06906927703972196]}
Episode 9, Step episode:
  Actions: [array([-0.99963063, -0.9996775 ], dtype=float32), array([-0.99998075,  0.9996871 ], dtype=float32), array([-0.999327 ,  0.9996466], dtype=float32)]
  Rewards: [-1.3135623482057146, -1.5727088314725024, -0.9274099862569823]
Episode 10, Step episode:
  Actions: [array([-0.9996016, -0.9996883], dtype=float32), array([-0.99998355,  0.9997485 ], dtype=float32), array([-0.99940866,  0.99961   ], dtype=float32)]
  Rewards: [-1.3144463502981927, -1.1954864884636938, -0.9395216777590807]
Episode 20, Step episode:
  Actions: [array([-0.9998948, -0.9999408], dtype=float32), array([-0.9999941 ,  0.99993074], dtype=float32), array([-0.999791 ,  0.9997892], dtype=float32)]
  Rewards: [-1.3815393718630773, -0.08070528432948076, -1.0917403037579572]
Episode 30, Step episode:
  Actions: [array([-0.999841 , -0.9999064], dtype=float32), array([-0.9999963,  0.9999548], dtype=float32), array([-0.9998866 ,  0.99968785], dtype=float32)]
  Rewards: [-1.477132841861974, -0.13649042271380307, 0.10823712923307771]
Episode 40, Step episode:
  Actions: [array([-0.9998273 , -0.99988484], dtype=float32), array([-0.9999951 ,  0.99994004], dtype=float32), array([-0.99990284,  0.99964434], dtype=float32)]
  Rewards: [-1.504362076730979, -0.623575507467026, -0.01742843297837271]
Episode 50, Step episode:
  Actions: [array([-0.9998039 , -0.99985373], dtype=float32), array([-0.9999936 ,  0.99992734], dtype=float32), array([-0.9999106,  0.9995927], dtype=float32)]
  Rewards: [-1.5052509286712255, -0.7852361931470921, -0.16621816603994616]
Episode episode - Completed
  Reward: -2.4567
  Success: False
  Steps: 50
  ------------------------------------------------------------

