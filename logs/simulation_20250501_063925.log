Multi-UAV Simulation Log - Started at 20250501_063925
================================================================================

Configuration Parameters:
  scenario_width: 10.0
  scenario_height: 10.0
  uav_radius: 0.05
  target_radius: 0.05
  obstacle_radius_min: 0.04
  obstacle_radius_max: 0.1
  capture_distance: 1.2
  sensor_range: 0.4
  max_steps_per_episode: 50
  num_sensors: 24
  trajectory_length: 50
  time_step: 1
  uav_initial_velocity: 0.0
  uav_max_velocity: 0.13
  uav_max_acceleration: 0.05
  uav_mass: 0.5
  target_initial_velocity: 0.0
  target_max_velocity: 0.13
  target_max_acceleration: 0.05
  replay_buffer_size: 100000
  batch_size: 256
  pre_batch_size: 50
  gamma: 0.99
  tau: 0.001
  actor_lr: 0.0005
  critic_lr: 0.001
  num_episodes: 30000
  epsilon: 1.0
  epsilon_min: 0.01
  epsilon_decay: 0.98
  d_limit: 1.5
  curriculum_threshold: 5000
  curriculum_step_size: 1000
  curriculum_learning: {'reward_weights': {'approach': 1.0, 'safety': 0.5, 'track': 0.3, 'encircle': 0.5, 'capture': 1.0, 'finish': 10.0}, 'curriculum_steps': [{'step': 0, 'reward_weights': {'approach': 1.0, 'safety': 0.5, 'track': 0.3, 'encircle': 0.5, 'capture': 1.0, 'finish': 10.0}}, {'step': 1000, 'reward_weights': {'approach': 0.8, 'safety': 0.6, 'track': 0.4, 'encircle': 0.6, 'capture': 1.2, 'finish': 12.0}}, {'step': 2000, 'reward_weights': {'approach': 0.6, 'safety': 0.7, 'track': 0.5, 'encircle': 0.7, 'capture': 1.4, 'finish': 14.0}}, {'step': 3000, 'reward_weights': {'approach': 0.4, 'safety': 0.8, 'track': 0.6, 'encircle': 0.8, 'capture': 1.6, 'finish': 16.0}}, {'step': 4000, 'reward_weights': {'approach': 0.2, 'safety': 0.9, 'track': 0.7, 'encircle': 0.9, 'capture': 1.8, 'finish': 18.0}}]}
  correlation_weights: {'sigma1': 100, 'sigma2': 2, 'sigma3': 5}
  reward_weights: {'approach': 1.0, 'safety': 1.0, 'track': 1.0, 'encircle': 1.0, 'capture': 1.0, 'finish': 1.0}

Episode 0 - Started (Seed: 8965)
  Initial Positions:
    seed: 8965
ERROR: Curriculum learning: Episode 0, placing target closer to UAVs (min_distance=0.25)
Episode 0 - Started (Seed: 8965)
  Initial Positions:
    uavs: [[2.6760805555480287, 2.395144325097137], [6.380399608989691, 0.41279867141734494], [4.853557906955416, 5.040283181771522]]
    target: [1.769564405656465, 4.241891332214038]
    obstacles: {'positions': [[3.294398781727327, 0.4781846268431711], [9.311128751252657, 0.3447705379561204], [4.522327229252712, 8.398280073237995]], 'radii': [0.08122026369989511, 0.08532977763526645, 0.0796733278463978]}
Episode 0 - Started (Seed: 8965)
  Initial Positions:
    seed: 8965
ERROR: Curriculum learning: Episode 0, placing target closer to UAVs (min_distance=0.25)
Episode 0 - Started (Seed: 8965)
  Initial Positions:
    uavs: [[2.6760805555480287, 2.395144325097137], [6.380399608989691, 0.41279867141734494], [4.853557906955416, 5.040283181771522]]
    target: [1.769564405656465, 4.241891332214038]
    obstacles: {'positions': [[3.294398781727327, 0.4781846268431711], [9.311128751252657, 0.3447705379561204], [4.522327229252712, 8.398280073237995]], 'radii': [0.08122026369989511, 0.08532977763526645, 0.0796733278463978]}
Episode 10, Step episode:
  Actions: [array([-0.29705644,  1.        ], dtype=float32), array([-0.99551684,  0.8951618 ], dtype=float32), array([-1.        , -0.05627434], dtype=float32)]
  Rewards: [-0.9255943173416925, -0.9716656626454208, -1.344972063384065]
Episode 20, Step episode:
  Actions: [array([ 0.00375726, -0.02600659], dtype=float32), array([-1.,  1.], dtype=float32), array([-0.5617734,  1.       ], dtype=float32)]
  Rewards: [-1.1966321830897941, -0.41074482856493, -1.446594834155665]
Episode 30, Step episode:
  Actions: [array([-1.,  1.], dtype=float32), array([-0.36394465,  1.        ], dtype=float32), array([-1.        , -0.13150129], dtype=float32)]
  Rewards: [-1.2289835716881048, -0.5300780708276547, -1.462527738398955]
Episode 40, Step episode:
  Actions: [array([-0.24248405,  1.        ], dtype=float32), array([0.14077517, 1.        ], dtype=float32), array([-0.3005934,  1.       ], dtype=float32)]
  Rewards: [-1.3483564646423287, -0.694869154659485, -1.5414485661380213]
Episode 45, Step episode:
  Actions: [array([ 1.       , -0.5896515], dtype=float32), array([0.2952662 , 0.06475417], dtype=float32), array([-1.       ,  0.5412548], dtype=float32)]
  Rewards: [-1.3425992252633123, -0.8669097531474579, -1.4626014124925968]
Episode 50, Step episode:
  Actions: [array([-0.6265681,  0.717241 ], dtype=float32), array([0.09590673, 0.38679183], dtype=float32), array([-0.49344692,  1.        ], dtype=float32)]
  Rewards: [-1.3424590430857282, -0.44390648744168576, 0.1367009712838092]
Episode episode - Completed
  Reward: -1.6497
  Success: False
  Steps: 50
  ------------------------------------------------------------

