Multi-UAV Simulation Log - Started at 20250501_064329
================================================================================

Configuration Parameters:
  scenario_width: 10.0
  scenario_height: 10.0
  uav_radius: 0.05
  target_radius: 0.05
  obstacle_radius_min: 0.04
  obstacle_radius_max: 0.1
  capture_distance: 1.2
  sensor_range: 0.4
  max_steps_per_episode: 50
  num_sensors: 24
  trajectory_length: 50
  time_step: 1
  uav_initial_velocity: 0.0
  uav_max_velocity: 0.13
  uav_max_acceleration: 0.05
  uav_mass: 0.5
  target_initial_velocity: 0.0
  target_max_velocity: 0.13
  target_max_acceleration: 0.05
  replay_buffer_size: 100000
  batch_size: 256
  pre_batch_size: 50
  gamma: 0.99
  tau: 0.001
  actor_lr: 0.0005
  critic_lr: 0.001
  num_episodes: 30000
  epsilon: 1.0
  epsilon_min: 0.01
  epsilon_decay: 0.98
  d_limit: 1.5
  curriculum_threshold: 5000
  curriculum_step_size: 1000
  curriculum_learning: {'reward_weights': {'approach': 1.0, 'safety': 0.5, 'track': 0.3, 'encircle': 0.5, 'capture': 1.0, 'finish': 10.0}, 'curriculum_steps': [{'step': 0, 'reward_weights': {'approach': 1.0, 'safety': 0.5, 'track': 0.3, 'encircle': 0.5, 'capture': 1.0, 'finish': 10.0}}, {'step': 1000, 'reward_weights': {'approach': 0.8, 'safety': 0.6, 'track': 0.4, 'encircle': 0.6, 'capture': 1.2, 'finish': 12.0}}, {'step': 2000, 'reward_weights': {'approach': 0.6, 'safety': 0.7, 'track': 0.5, 'encircle': 0.7, 'capture': 1.4, 'finish': 14.0}}, {'step': 3000, 'reward_weights': {'approach': 0.4, 'safety': 0.8, 'track': 0.6, 'encircle': 0.8, 'capture': 1.6, 'finish': 16.0}}, {'step': 4000, 'reward_weights': {'approach': 0.2, 'safety': 0.9, 'track': 0.7, 'encircle': 0.9, 'capture': 1.8, 'finish': 18.0}}]}
  correlation_weights: {'sigma1': 100, 'sigma2': 2, 'sigma3': 5}
  reward_weights: {'approach': 1.0, 'safety': 1.0, 'track': 1.0, 'encircle': 1.0, 'capture': 1.0, 'finish': 1.0}

Episode 0 - Started (Seed: 9209)
  Initial Positions:
    seed: 9209
ERROR: Curriculum learning: Episode 0, placing target closer to UAVs (min_distance=0.25)
Episode 0 - Started (Seed: 9209)
  Initial Positions:
    uavs: [[3.543453477610355, 1.88426451302401], [6.123869226210351, 4.912875101266431], [0.566388591803209, 5.361362015717872]]
    target: [9.58885333290524, 2.7851276255230286]
    obstacles: {'positions': [[0.6455909438883348, 3.0759629380033755], [5.795751370578429, 3.1088961480920334], [2.0842958772963627, 7.817569349488375]], 'radii': [0.0939472026941914, 0.08033406890854897, 0.07309522751660685]}
Episode 0 - Started (Seed: 9209)
  Initial Positions:
    seed: 9209
ERROR: Curriculum learning: Episode 0, placing target closer to UAVs (min_distance=0.25)
Episode 0 - Started (Seed: 9209)
  Initial Positions:
    uavs: [[3.543453477610355, 1.88426451302401], [6.123869226210351, 4.912875101266431], [0.566388591803209, 5.361362015717872]]
    target: [9.58885333290524, 2.7851276255230286]
    obstacles: {'positions': [[0.6455909438883348, 3.0759629380033755], [5.795751370578429, 3.1088961480920334], [2.0842958772963627, 7.817569349488375]], 'radii': [0.0939472026941914, 0.08033406890854897, 0.07309522751660685]}
Episode 10, Step episode:
  Actions: [array([-1.        , -0.28389367], dtype=float32), array([-1.        , -0.05833029], dtype=float32), array([-1.       , -0.5964739], dtype=float32)]
  Rewards: [0.5355948292220483, -0.10530985165491613, 0.304979650072887]
Episode 20, Step episode:
  Actions: [array([0.4591363, 1.       ], dtype=float32), array([-0.6773588, -0.5052494], dtype=float32), array([-0.6772319 , -0.46875137], dtype=float32)]
  Rewards: [-0.967489580596202, -0.5690360340924222, 0.11244630057259686]
Episode 30, Step episode:
  Actions: [array([0.15706256, 1.        ], dtype=float32), array([1.        , 0.16831554], dtype=float32), array([ 0.63066477, -1.        ], dtype=float32)]
  Rewards: [-1.0282377825680793, -0.9752764059882681, -0.1799229317481747]
Episode 34, Step episode:
  Actions: [array([-0.28268924,  1.        ], dtype=float32), array([0.73789746, 1.        ], dtype=float32), array([1., 1.], dtype=float32)]
  Rewards: [-1.3746328120232796, -2.321863060092442, -0.22760094364879058]
Episode 40, Step episode:
  Actions: [array([-1.,  1.], dtype=float32), array([-1.,  1.], dtype=float32), array([-0.86194074, -0.10589554], dtype=float32)]
  Rewards: [-1.6890062338425404, -1.1036017207942845, -0.29156388886795614]
Episode 50, Step episode:
  Actions: [array([-0.87783164,  0.8640476 ], dtype=float32), array([ 1., -1.], dtype=float32), array([ 1.        , -0.20624158], dtype=float32)]
  Rewards: [-1.318009510019905, -1.2476356488770644, -0.5017067743129154]
Episode episode - Completed
  Reward: -3.0674
  Success: False
  Steps: 50
  ------------------------------------------------------------

